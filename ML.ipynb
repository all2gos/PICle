{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b864e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#we need to load proper data sets\n",
    "#all.csv contains all individual clusters\n",
    "all_df = pd.read_csv('all.csv')\n",
    "\n",
    "#my_e contains a list of clusters for which \n",
    "#I determined the energy using the developed approximation techniques\n",
    "my_e = pd.read_csv('my_e.csv').drop(['Unnamed: 0','SCF_steps','magnetism',\n",
    "                                     'time_of_calculation'],axis=1)\n",
    "#clusters_phd.csv contains a bunch of clusters with precisely determined energy\n",
    "#however, not all of the cases in this set were determined also by me\n",
    "#the common part was included in the phd_common_part\n",
    "#THIS DATASET has an energy predicted by me\n",
    "phd_aprox = pd.read_csv('phd_common_part.csv').drop(['Unnamed: 0','SCF_steps','magnetism',\n",
    "                                               'time_of_calculation'],axis=1)\n",
    "#AND THIS DATASET has an energy calculated by phd_student\n",
    "full_real_phd = pd.read_csv('clusters_phd.csv')[['full_notation','energy']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1fd76",
   "metadata": {},
   "source": [
    "### Data Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all of phd_aprox, my_e and full_real_phd do not contain any information that could be used by the model\n",
    "#this information is in the all.csv file, so it must be extracted and assigned \n",
    "#to the appropriate rows in the mentioned data sets\n",
    "\n",
    "\n",
    "def topology_to_list(imported_topology):\n",
    "    topology = []\n",
    "    for i in range(2,80,6):\n",
    "        topology.append(imported_topology[i:i+2])        \n",
    "    return topology\n",
    "\n",
    "def extracting(df):\n",
    "    #I want to load some information from all.csv\n",
    "    \n",
    "    #splitting \"full_notation\" column\n",
    "    df = df.assign(number = df['full_notation'].apply(lambda x: x.split('_')[0]),\n",
    "              joining_type = df['full_notation'].apply(lambda x: x.split('_')[1]),\n",
    "              first_atom = df['full_notation'].apply(lambda x: x.split('_')[2]),\n",
    "              second_atom = df['full_notation'].apply(lambda x: x.split('_')[3]),\n",
    "            no_nickel = all_df['no_nickel'].apply(lambda x: x))\n",
    "        \n",
    "    df['no_nickel'], df['c_atom'],df['mass_center'],df['shortest_paths'] = None,None,None,None\n",
    "    df['bonds'], df['comp_of_sphere'], df['hydrogen'], df['topology'] = None, None, None, None\n",
    "    df['type_of_first_atom'], df['type_of_second_atom'] = None,None\n",
    "    df['energy_wth_h2'] = df['energy']\n",
    "    \n",
    "    for cluster in range(len(df)):\n",
    "        for column in ['no_nickel','c_atom','mass_center','shortest_paths','comp_of_sphere','bonds']:\n",
    "            df[column].iloc[cluster] = all_df[column].loc[int(df['number'].iloc[cluster])]\n",
    "        for column in ['topology']:\n",
    "            df[column].iloc[cluster] = topology_to_list(all_df[column].loc[int(df['number'].iloc[cluster])])\n",
    "        df['hydrogen'].iloc[cluster] = 1 if df['joining_type'].iloc[cluster] in ['MOCOM','MOCMHnbond','MC'] else 0\n",
    "        if df['joining_type'].iloc[cluster] in ['MOCOM','MOCMHnbond','MC']:\n",
    "            df['energy_wth_h2'].iloc[cluster] += 3.3831\n",
    "\n",
    "    for cluster in range(len(df)):\n",
    "        df['type_of_first_atom'].iloc[cluster] = df['topology'].iloc[cluster][int(df['first_atom'].iloc[cluster])]\n",
    "        df['type_of_second_atom'].iloc[cluster] = df['topology'].iloc[cluster][int(df['second_atom'].iloc[cluster])]\n",
    "    \n",
    "    df = df.assign(ni_ncentre = df['shortest_paths'].apply(lambda x:x.split(',')[0][1:]),\n",
    "              ni_centre = df['shortest_paths'].apply(lambda x:x.split(',')[1]),\n",
    "               cu_ncentre = df['shortest_paths'].apply(lambda x:x.split(',')[2]),\n",
    "                cu_centre = df['shortest_paths'].apply(lambda x:x.split(',')[3][:-1]),\n",
    "                ni0 = df['comp_of_sphere'].apply(lambda x: x.split(',')[0][1:]),\n",
    "               ni8 = df['comp_of_sphere'].apply(lambda x: x.split(',')[1]),\n",
    "               ni17 = df['comp_of_sphere'].apply(lambda x: x.split(',')[2]),\n",
    "               ni25 = df['comp_of_sphere'].apply(lambda x: x.split(',')[3]),\n",
    "               ni33 = df['comp_of_sphere'].apply(lambda x: x.split(',')[4]),\n",
    "               ni42 = df['comp_of_sphere'].apply(lambda x: x.split(',')[5]),\n",
    "               ni50 = df['comp_of_sphere'].apply(lambda x: x.split(',')[6][:-1]),\n",
    "               nini = df['bonds'].apply(lambda x: x.split(',')[0][1:]),\n",
    "               cucu = df['bonds'].apply(lambda x: x.split(',')[1]),\n",
    "               nicu = df['bonds'].apply(lambda x: x.split(',')[2][:-1]))\n",
    "    \n",
    "    return df.drop(['shortest_paths','bonds','comp_of_sphere','topology'],axis=1)\n",
    "    \n",
    "def energy_mix(df, reverse=False,new_name='mix_energy'):\n",
    "    #function that transcribe energy to mixed energy\n",
    "\n",
    "    #first value is nickel energy, second is cooper\n",
    "\n",
    "    monomet_cl_energy = {'MO':(-74.568295,-57.156901),\n",
    "                                            'MOCOM':(-79.855113,-62.216739),\n",
    "                                            'MOCMHnbond':(-79.004707,-61.301059),\n",
    "                                            'MC':(-78.378661,-60.821898),\n",
    "                                            'MOCM':(-75.478348,-57.428965)}\n",
    "    if 'no_nickel' in df.columns and 'joining_type' in df.columns:\n",
    "        print('Appriopiate columns are present in database')\n",
    "        df[new_name] = None\n",
    "        for i in range(len(df)):\n",
    "            \n",
    "            nickel_part = monomet_cl_energy[df['joining_type'].iloc[i]][0]\n",
    "            copper_part = monomet_cl_energy[df['joining_type'].iloc[i]][1]\n",
    "            nickel = int(df['no_nickel'].iloc[i])   \n",
    "            if reverse == True:\n",
    "                mix_energy = df['predicted'].iloc[i]\n",
    "                df[new_name].iloc[i] = round((13*mix_energy + (1/13)*(nickel*nickel_part + (13-nickel)*copper_part)),6)\n",
    "            else:   \n",
    "                energy = df['energy'].iloc[i]     \n",
    "                df[new_name].iloc[i] = round((1/13)*(energy - (1/13)*(nickel*nickel_part + (13-nickel)*copper_part)),6)\n",
    "        return df\n",
    "    else:\n",
    "        print('Given database is missing at least one column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610be4aa",
   "metadata": {},
   "source": [
    "### ML section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks to above code I can generate good data sets by only one line\n",
    "\n",
    "my_e = energy_mix(extracting(my_e))\n",
    "phd_aprox = energy_mix(extracting(phd_aprox))\n",
    "full_real_phd = energy_mix(extracting(full_real_phd))\n",
    "\n",
    "#my_e = my_e[my_e['no_nickel'] <7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc81af",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#g1 = sns.scatterplot(data=my_e.sort_values(by='no_nickel'), x='full_notation',y=\"mix_energy\", hue='c_atom')\n",
    "#g1.set(xlabel='continous cases', ylabel = 'Normalize energy', xticklabels=[], title = 'Normalize energy for all cases')\n",
    "#plt.legend(title='Central Atom', labels =['Cu','Ni'])\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_order = ['full_notation', 'energy','energy_wth_h2','mix_energy',\n",
    "        'number', 'joining_type', 'first_atom',\n",
    "       'second_atom', 'no_nickel', 'mass_center', 'hydrogen',        \n",
    "       'ni_ncentre', 'ni_centre', 'cu_ncentre', 'cu_centre', 'ni0', 'ni8',\n",
    "       'ni17', 'ni25', 'ni33', 'ni42', 'ni50', 'nini', 'cucu', 'nicu','c_atom',\n",
    "        'type_of_first_atom', 'type_of_second_atom']\n",
    "\n",
    "my_e = pd.DataFrame(data=my_e, columns = fine_order )\n",
    "\n",
    "phd_aprox = pd.DataFrame(data=phd_aprox, columns =fine_order)\n",
    "\n",
    "full_real_phd = pd.DataFrame(data=full_real_phd, columns = fine_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(data = my_e, columns = ['type_of_first_atom','type_of_second_atom'], drop_first=True)\n",
    "plt.subplots(figsize=(14,14))\n",
    "X = X.drop(['full_notation','joining_type', 'number','first_atom','second_atom'],axis=1)\n",
    "X = X.astype('float64')\n",
    "\n",
    "\n",
    "#we can see that all the parameters related to the creation of a path after the cluster are highly correlated with each other so \n",
    "#I will leave the one most correlated with energy: cu_ncentre \n",
    "\n",
    "X= X.drop(['cu_centre','ni_centre','ni_ncentre'],axis=1)\n",
    "\n",
    "sns.heatmap(((X.corr()*100)),annot=True,cmap='coolwarm', cbar=False)\n",
    "\n",
    "f = plt.savefig('heatmap', bbox_inches='tight')\n",
    "\n",
    "#I have to do exaclty the same steps for phf ds as for my_e \n",
    "\n",
    "phd_aprox = pd.get_dummies(data = phd_aprox, columns = ['type_of_first_atom','type_of_second_atom'], drop_first=True)\n",
    "phd_aprox = phd_aprox.drop(['full_notation','joining_type', 'number','first_atom','second_atom','cu_centre','ni_centre','ni_ncentre'],axis=1)\n",
    "phd_aprox = phd_aprox.astype('float64')\n",
    "\n",
    "full_real_phd = pd.get_dummies(data = full_real_phd, columns = ['type_of_first_atom','type_of_second_atom'], drop_first=True)\n",
    "full_real_phd = full_real_phd.drop(['full_notation','joining_type', 'number','first_atom','second_atom','cu_centre','ni_centre','ni_ncentre'],axis=1)\n",
    "full_real_phd = full_real_phd.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's normalize data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_norm = X.drop(['energy','energy_wth_h2','mix_energy'],axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_norm)\n",
    "\n",
    "X_norm = scaler.transform(X_norm)\n",
    "\n",
    "phd_norm = scaler.transform(phd_aprox.drop(['energy','energy_wth_h2','mix_energy'],axis=1))\n",
    "full_real_phd_norm = scaler.transform(full_real_phd.drop(['energy','energy_wth_h2','mix_energy'],axis=1))\n",
    "\n",
    "full_real_phd_norm = pd.DataFrame(data = full_real_phd_norm, columns = full_real_phd.drop(['energy','energy_wth_h2','mix_energy'],axis=1).columns)\n",
    "phd_norm = pd.DataFrame(data = phd_norm, columns = phd_aprox.drop(['energy','energy_wth_h2','mix_energy'],axis=1).columns)\n",
    "X_norm = pd.DataFrame(data = X_norm, columns = X.drop(['energy','energy_wth_h2','mix_energy'],axis=1).columns)\n",
    "\n",
    "\n",
    "#let's now deal with outliers\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "result = pd.concat([X_norm, X[['energy','energy_wth_h2','mix_energy']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "val_phd_set = pd.concat([phd_norm, phd_aprox[['energy','energy_wth_h2','mix_energy']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "val_full_real_phd = pd.concat([full_real_phd_norm.reset_index(drop=True), full_real_phd[['energy','energy_wth_h2','mix_energy']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "#X_after_out = result[(np.abs(stats.zscore(result)) < 2).all(axis=1)]\n",
    "X_after_out = result\n",
    "\n",
    "#and create train and test data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = X_after_out['energy']\n",
    "y = np.asarray(y).astype('float32')\n",
    "X_norm = X_after_out.drop(['energy','energy_wth_h2','mix_energy'],axis=1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_norm,y,test_size=0.3, random_state=42)\n",
    "\n",
    "X_after_out['no_nickel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c55091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from joblib import dump, load\n",
    "\n",
    "def standard_accuracy(predicitons, real_data):\n",
    "\n",
    "    avg_error = 0  \n",
    "    for i in range(len(predicitons)):\n",
    "        avg_error+= np.sqrt((predicitons[i]-real_data[i])**2)            \n",
    "    avg_error /= len(predicitons)   \n",
    "\n",
    "    return 1-(avg_error/(max(real_data)-min(real_data)))   \n",
    "\n",
    "def entropy_index(predicitons, real_data):\n",
    "    \n",
    "\n",
    "    val_df = pd.DataFrame(data=[predicitons,real_data]).transpose()\n",
    "    val_df = val_df.sort_values(by=[val_df.columns[0]]).reset_index(names='reset').reset_index(names='pred').sort_values(by=[val_df.columns[1]]).reset_index(names='reset1').reset_index(names='real')\n",
    "\n",
    "    entr = 0\n",
    "    max_diff = 0\n",
    "    for i in range(len(val_df)):\n",
    "        diff = np.sqrt((val_df['pred'][i]-val_df['real'][i])**2)\n",
    "        entr += diff\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "    return entr/(len(val_df)), max_diff\n",
    "\n",
    "\n",
    "def krr(X_train,X_test,y_train,y_test, alphas = [0.2,0.4,0.6,0.8,1,1.5,2,3,4,5,8,12,15,20], degrees = [x for x in range(1,8)], kernel = ['linear', 'poly', 'rbf', 'sigmoid'], accuracy='entropy'):\n",
    "    \n",
    "    best_acc = [0,0,0]\n",
    "    for k in kernel:\n",
    "        for alpha in alphas:\n",
    "            for degree in degrees:\n",
    "                \n",
    "                model = KernelRidge(alpha=alpha,degree = degree, kernel = k)\n",
    "                model.fit(X_train, y_train)\n",
    "                model_pred = model.predict(X_test)\n",
    "                \n",
    "                if accuracy == 'entropy':\n",
    "                    acc = 1/entropy_index(model_pred,y_test)[0]\n",
    "                else:\n",
    "                    acc = standard_accuracy(model_pred, y_test)\n",
    "\n",
    "                if acc > best_acc[2]:\n",
    "                    best_acc = ['best krr model,','accuracy:',round((1/acc)/len(y_test),2), 'max_diff:',round(entropy_index(model_pred,y_test)[1]/len(y_test),2), 'alpha: ', alpha,'degree:', degree, 'kernel:',k]\n",
    "                    dump(model, 'model_krr.joblib') \n",
    "    for p in best_acc:\n",
    "        print(p,end=' ')\n",
    "    print()\n",
    "    return(best_acc, y_test, model_pred) \n",
    "\n",
    "\n",
    "def svr(X_train,X_test,y_train,y_test, C_parameter = [0.2,0.4,0.6,0.8,1,1.5,2,3,4,5,8,12,15,20], degrees = [x for x in range(1,8)], epsilon = [x/10 for x in range(1,12)], kernel = ['linear', 'poly', 'rbf', 'sigmoid'], accuracy='entropy'):\n",
    "    \n",
    "    best_acc = [0,0,0]\n",
    "    for k in kernel:\n",
    "        for C in C_parameter:\n",
    "            for degree in degrees:\n",
    "                for eps in epsilon:\n",
    "                \n",
    "                    model = SVR(C=C,degree = degree, epsilon = eps, kernel = k)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    model_pred = model.predict(X_test)\n",
    "                    \n",
    "                    if accuracy == 'entropy':\n",
    "                        acc = 1/entropy_index(model_pred,y_test)[0]\n",
    "                    else:\n",
    "                        acc = standard_accuracy(model_pred, y_test)\n",
    "\n",
    "\n",
    "                    if acc > best_acc[2]:\n",
    "                        best_acc = ['best svr model,','accuracy:',round((1/acc)/len(y_test),2), 'max_diff:',round(entropy_index(model_pred,y_test)[1]/len(y_test),2), 'C: ', C,'degree:', degree, 'eps:',eps, 'kernel:',k]\n",
    "                        dump(model, 'model_svr.joblib') \n",
    "    for p in best_acc:\n",
    "        print(p,end=' ')\n",
    "    print()\n",
    "    return(best_acc, y_test, model_pred) \n",
    "\n",
    "def rf(X_train,X_test,y_train,y_test, estimators= [x* 25 for x in range(1,20)], min_samples_split = [x for x in range(2,7)], max_depth = [2,3,4,5,6,None], accuracy = 'entropy'):\n",
    "    \n",
    "    best_acc = [0,0,0]\n",
    "    for min in min_samples_split:\n",
    "        for est in estimators:\n",
    "            for depth in max_depth:\n",
    "\n",
    "                model = RandomForestRegressor(n_estimators=est, min_samples_split=min, max_depth=depth)  \n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                model_pred = model.predict(X_test)\n",
    "\n",
    "                if accuracy == 'entropy':\n",
    "                    acc = 1/entropy_index(model_pred,y_test)[0]\n",
    "                else:\n",
    "                    acc = standard_accuracy(model_pred, y_test)\n",
    "\n",
    "                if acc > best_acc[2]:\n",
    "                    best_acc = ['best rf model,','accuracy:',round((1/acc)/len(y_test),2), 'max_diff:',round(entropy_index(model_pred,y_test)[1]/len(y_test),2), 'est:' , est, 'minimum_sample_split:',min, 'max_depth',depth]\n",
    "                    dump(model, 'model_rf.joblib') \n",
    "        \n",
    "    for p in best_acc:\n",
    "        print(p,end=' ')\n",
    "    print()\n",
    "\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    print('feature importance RF:')\n",
    "    for feature, score in zip(X_test.columns, importance):\n",
    "        print(f\"{feature}: {round(score,3)}\")\n",
    "    \n",
    "    return(best_acc, y_test, model_pred)\n",
    "\n",
    "def etr(X_train,X_test,y_train,y_test, estimators= [x* 25 for x in range(1,20)],min_samples_split = [x for x in range(2,7)], max_depth = [2,3,4,5,6,None], accuracy = 'entropy'):\n",
    "    \n",
    "    best_acc = [0,0,0]\n",
    "    for est in estimators:\n",
    "        \n",
    "        for min in min_samples_split:\n",
    "            for depth in max_depth:\n",
    "                model = ExtraTreesRegressor(n_estimators=est, min_samples_split=min, max_depth=depth)  \n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                model_pred = model.predict(X_test)\n",
    "\n",
    "                if accuracy == 'entropy':\n",
    "                    acc = 1/entropy_index(model_pred,y_test)[0]\n",
    "                else:\n",
    "                    acc = standard_accuracy(model_pred, y_test)\n",
    "                        \n",
    "                if acc > best_acc[2]:\n",
    "                    best_acc = ['best etr model,','accuracy:',round((1/acc)/len(y_test),2), 'max_diff:',round(entropy_index(model_pred,y_test)[1]/len(y_test),2), 'est:' , est,'min_sample_split:',min, 'max_depth:',depth]\n",
    "                    dump(model, 'model_etr.joblib') \n",
    "        \n",
    "    for p in best_acc:\n",
    "        print(p,end=' ')\n",
    "    print()\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    print('feature importance ETR:')\n",
    "    for feature, score in zip(X_test.columns, importance):\n",
    "        print(f\"{feature}: {round(score,3)}\")\n",
    "\n",
    "    \n",
    "    return(best_acc, y_test, model_pred)\n",
    "\n",
    "def gbt(X_train,X_test,y_train,y_test, estimators= [x* 25 for x in range(1,15)], lr = [0.2,0.4,0.6,0.8,1,1.5,2,3],min_samples_split = [x for x in range(2,5)], max_depth = [2,3,4,5,None], subsample = [x/10 for x in range(7,11)], loss_f = ['squared_error', 'absolute_error', 'huber', 'quantile'],accuracy = 'entropy'):\n",
    "    print('---------GBT---------')\n",
    "    best_acc = [0]\n",
    "    for est in estimators:\n",
    "        for l in lr:\n",
    "            for min in min_samples_split:\n",
    "                for subs in subsample:\n",
    "                    for depth in max_depth:\n",
    "                        for loss in loss_f:\n",
    "                            \n",
    "                            print(est,l,min,subs,depth,loss)\n",
    "                            model = GradientBoostingRegressor(n_estimators=est, learning_rate=l, min_samples_split=min, max_depth=depth, subsample=subs, loss=loss)  \n",
    "                            \n",
    "                            model.fit(X_train, y_train)\n",
    "                            model_pred = model.predict(X_test)\n",
    "\n",
    "                            if accuracy == 'entropy':\n",
    "                                acc = 1/entropy_index(model_pred,y_test)[0]\n",
    "                            else:\n",
    "                                acc = standard_accuracy(model_pred, y_test)\n",
    "                                    \n",
    "                            if acc > best_acc[0]:\n",
    "                                best_acc = [1/acc, 'max_diff:',entropy_index(model_pred,y_test)[1],'gbr', 'est:' , est, 'learning rate:',l,'min_sample_split:',min, 'max_depth:',depth, 'subsample:',subs, 'loss_f:',loss]\n",
    "                                dump(model, 'model_gbt.joblib') \n",
    "    print('-------GBT RESULTS------------')\n",
    "    print(best_acc) \n",
    "    \n",
    "    return(best_acc, y_test, model_pred)\n",
    "\n",
    "def ada(X_train,X_test,y_train,y_test, estimators= [x*25 for x in range(1,20)], lr = [0.2,0.4,0.6,0.8,1,1.5,2,3], loss_f = ['linear', 'square', 'exponential'], accuracy = 'entropy'):#\n",
    "    \n",
    "    best_acc = [0,0,0]\n",
    "    for est in estimators:\n",
    "        for learning_rate in lr:  \n",
    "            for loss in loss_f:\n",
    "        \n",
    "                model = AdaBoostRegressor(n_estimators=est, learning_rate=learning_rate, loss=loss)    \n",
    "                model.fit(X_train, y_train)\n",
    "                model_pred = model.predict(X_test)\n",
    "\n",
    "                if accuracy == 'entropy':\n",
    "                    acc = 1/entropy_index(model_pred,y_test)[0]\n",
    "                else:\n",
    "                    acc = standard_accuracy(model_pred, y_test)\n",
    "                \n",
    "                if acc > best_acc[2]:\n",
    "                    best_acc = ['best ada model,','accuracy:',round((1/acc)/len(y_test),2), 'max_diff:',round(entropy_index(model_pred,y_test)[1]/len(y_test),2),  'est:',est, 'learning rate:',learning_rate, 'loss:',loss]\n",
    "                    dump(model, 'model_ada.joblib') \n",
    "\n",
    "    for p in best_acc:\n",
    "        print(p,end=' ')\n",
    "    print()     \n",
    "\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    print('feature importance ADA:')\n",
    "    for feature, score in zip(X_test.columns, importance):\n",
    "        print(f\"{feature}: {round(score,3)}\")\n",
    "        \n",
    "    return(best_acc, y_test, model_pred)\n",
    "\n",
    "\n",
    "#svr_m = svr(X_train,X_test,y_train,y_test)\n",
    "krr_m = krr(X_train,X_test,y_train,y_test)\n",
    "#ada_m = ada(X_train,X_test,y_train,y_test)\n",
    "#rf_m = rf(X_train,X_test,y_train,y_test)\n",
    "#etr_m = etr(X_train,X_test,y_train,y_test)\n",
    "#gbt(X_train,X_test,y_train,y_test)\n",
    "\n",
    "print('ML done')\n",
    "print('X_test len',len(X_test))\n",
    "print(X_test['no_nickel'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dee4c",
   "metadata": {},
   "source": [
    "### Data Visualisation and Validation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load('model_krr.joblib')\n",
    "\n",
    "col = 'energy_wth_h2'\n",
    "#show the dataset on which the model was trained and tested\n",
    "validate = pd.DataFrame(data = [best_model.predict(X_test),y_test]).transpose()\n",
    "entr = entropy_index(best_model.predict(X_test),y_test)\n",
    "print('dla testowego',round(entr[0]/len(y_test),3),round(entr[1]/len(y_test),3))\n",
    "\n",
    "#show dataset containing cases from clusters_phd file\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real_energy'})\n",
    "entr = entropy_index(best_model.predict(full_real_phd_norm),val_full_real_phd[col])\n",
    "print('dla real phd',round(entr[0]/len(val_full_real_phd),3),round(entr[1]/len(val_full_real_phd),3))\n",
    "\n",
    "colors = ['#e9aeae','#9b9bba']\n",
    "\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "markers = {'predicted': \"h\", 'real_energy': \"o\"}\n",
    "plt.figure(figsize=(2,2))\n",
    "ax = sns.scatterplot(data = validate.sort_values(by=['real_energy']).reset_index(drop=True), markers=markers)\n",
    "ax.set_xlabel(\"consecutive cases\")\n",
    "ax.set_ylabel(\"energy [eV]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('entropy_of_aprox_technique.csv')\n",
    "print('Entropy:',entropy_index(t['energy'],t['energy_aprox']))\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "markers = {'energy': \"h\", 'energy_aprox': \"o\"}\n",
    "ax = sns.scatterplot(data = t[['energy','energy_aprox']].sort_values(by=['energy_aprox']).reset_index(drop=True), markers=markers)\n",
    "ax.set_xlabel(\"consecutive cases\")\n",
    "ax.set_ylabel(\"energy [eV]\")\n",
    "f = plt.savefig('quality_of_aprox_technique.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb197e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#e9aeae','#9b9bba']\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "plt.figure(figsize=(20,12))\n",
    "markers = {'real energy': \"h\", 'predicted': \"o\"}\n",
    "\n",
    "best_model = load('allni_0z_13.joblib')\n",
    "col = 'mix_energy'\n",
    "plt.subplot(4, 3, 1)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "validate.to_csv(\"val_1.csv\")\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'mix_energy', xticklabels=[], title='allni_0z_mix')\n",
    "\n",
    "best_model = load('allni_0z_norm.joblib')\n",
    "col = 'energy'\n",
    "plt.subplot(4, 3, 2)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'energy_norm', xticklabels=[], title='allni_0z_norm')\n",
    "\n",
    "best_model = load('allni_0z_wth.joblib')\n",
    "col = 'energy_wth_h2'\n",
    "plt.subplot(4, 3, 3)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'energy_wth', xticklabels=[], title = 'allni_0z_wth')\n",
    "\n",
    "best_model = load('allni_2z_13.joblib')\n",
    "col = 'mix_energy'\n",
    "plt.subplot(4, 3, 4)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "validate.to_csv(\"val_2.csv\")\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'mix_energy', xticklabels=[], title='allni_2z_mix')\n",
    "\n",
    "best_model = load('allni_2z_norm.joblib')\n",
    "col = 'energy'\n",
    "plt.subplot(4, 3, 5)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'energy_norm', xticklabels=[], title = 'allni_2z_norm')\n",
    "\n",
    "best_model = load('allni_2z_wth.joblib')\n",
    "col = 'energy_wth_h2'\n",
    "plt.subplot(4, 3, 6)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'energy_wth', xticklabels=[], title='allni_2z_wth')\n",
    "\n",
    "best_model = load('ni7_0z_13.joblib')\n",
    "col = 'mix_energy'\n",
    "plt.subplot(4, 3, 7)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "validate.to_csv(\"val_3.csv\")\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'mix_energy', xticklabels=[], title='ni7_0z_mix')\n",
    "\n",
    "\n",
    "best_model = load('ni7_0z_norm.joblib')\n",
    "col = 'energy'\n",
    "plt.subplot(4, 3, 8)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'energy_norm', xticklabels=[], title='ni7_0z_norm')\n",
    "\n",
    "\n",
    "best_model = load('ni7_0z_wth.joblib')\n",
    "col = 'energy_wth_h2'\n",
    "plt.subplot(4, 3, 9)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(ylabel = 'energy_wth', xticklabels=[], title='ni7_0z_wth')\n",
    "\n",
    "\n",
    "best_model = load('ni7_2z_13.joblib')\n",
    "col = 'mix_energy'\n",
    "plt.subplot(4, 3, 10)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "validate.to_csv(\"val_4.csv\")\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(xlabel='Continous cases in validation set (mix_energy)', ylabel = 'mix_energy', xticklabels=[], title='ni7_2z_mix')\n",
    "\n",
    "\n",
    "best_model = load('ni7_2z_norm.joblib')\n",
    "col = 'energy'\n",
    "plt.subplot(4, 3, 11)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers, legend=False)\n",
    "g1.set(xlabel='Continous cases in validation set (energy_norm)', ylabel = 'energy_norm', xticklabels=[], title='ni7_2z_norm')\n",
    "\n",
    "\n",
    "best_model = load('ni7_2z_wth.joblib')\n",
    "col = 'energy_wth_h2'\n",
    "plt.subplot(4, 3, 12)\n",
    "validate = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd[col]]).transpose().rename(columns = {0:'predicted',1:'real energy'})\n",
    "print(validate['predicted'].nunique(), validate['real energy'].nunique())\n",
    "g1 = sns.scatterplot(data = validate.sort_values(by=['real energy']).reset_index(drop=True), markers=markers)\n",
    "g1.set(xlabel='Continous cases in validation set (energy_wth_h2)', ylabel = 'energy_wth', xticklabels=[], title='ni7_2z_wth')\n",
    "\n",
    "f = plt.savefig('first_step_of_ml.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load('allni_0z_13.joblib')\n",
    "df = extracting(pd.read_csv('clusters_phd.csv')[['full_notation','energy']])\n",
    "\n",
    "test = pd.DataFrame(data = [best_model.predict(full_real_phd_norm),val_full_real_phd['mix_energy']]).transpose().rename(columns = {0:'predicted',1:'real_energy'})\n",
    "test = pd.concat([full_real_phd.reset_index(drop=True), df['joining_type'], test],axis=1)\n",
    "print((sum(test['mix_energy'] == test['real_energy'])/len(test)))\n",
    "test = energy_mix(test, reverse=True, new_name='energy_transform')\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tmix = entropy_index(test['energy'],test['energy_transform'])\n",
    "mix_mix = entropy_index(test['mix_energy'], test['predicted'])\n",
    "\n",
    "print(norm_tmix[0]/len(test),norm_tmix[1]/len(test))\n",
    "print(mix_mix[0]/len(test),mix_mix[1]/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "data = test[['energy','energy_transform']].sort_values(by=['energy']).rename(columns={'energy': 'norm', 'energy_transform':'t(mix)'}).reset_index(drop=True)\n",
    "print(data.columns)\n",
    "markers = {'norm': \"h\", 't(mix)': \"o\"}\n",
    "ax = sns.scatterplot(data = data, markers=markers)\n",
    "ax.set_xlabel(\"Continuous cases in validation set\", fontsize=16)\n",
    "ax.set_ylabel(\"Energy [eV]\", fontsize=16)\n",
    "f = plt.savefig('quality_of_aprox_technique.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
